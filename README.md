# Conversational AI Tutor  

An **AI-powered conversational tutor** with **RAG backend, Speech-to-Text (STT), Text-to-Speech (TTS), and an animated mascot frontend**.  
This project was built as part of an **AI assignment challenge** to demonstrate **end-to-end AI system design, API integration, and deployment**.  

1. **Live Demo**: [Conversational AI Tutor](https://conversational-ai-tutor.vercel.app/)
2. **Backend API**: [FastAPI Backend on Render](https://conversational-ai-tutor.onrender.com/)  

---

## Objective  

Build a **Conversational AI Tutor** that can:  
- Answer user queries using **RAG (Retrieval Augmented Generation)**.  
- Listen to the user via **speech recognition (STT)**.  
- Respond with voice via **text-to-speech (TTS)**.  
- Animate a mascot avatar with **lip-sync & emotions**.  

---

## Features  

**Backend (FastAPI + LangChain RAG + FAISS Vector DB)**  
- `/query` ‚Üí Single query answering  
- `/chat` ‚Üí Multi-turn conversational memory  
- Returns **answer + emotion state** (happy, thinking, explaining)  

**Speech Recognition (STT)**  
- Implemented using **OpenAI Whisper**  

**Text-to-Speech (TTS)**  
- Implemented using **pyttsx3 (offline TTS)**  

**Frontend (React)**  
- Animated **Mascot UI**  
- **Mic button** ‚Üí Speak ‚Üí STT ‚Üí Backend ‚Üí Mascot speaks & animates  
- Supports **lip-sync and emotion-based facial animations**  

**Deployment**  
- Backend deployed on **Render**  
- Frontend deployed on **Vercel**  

---

## Tech Stack  

- **Backend**: FastAPI, LangChain, FAISS, Whisper (STT), pyttsx3 (TTS)  
- **Frontend**: React.js, Web Audio API, Lottie/Canvas animations  
- **Deployment**: Render (Backend), Vercel (Frontend)  

---

## Project Structure  
```
conversational-ai-tutor/
‚îú‚îÄ‚îÄ backend/                                                     # FastAPI backend
‚îÇ ‚îú‚îÄ‚îÄ init.py
‚îÇ ‚îú‚îÄ‚îÄ main.py                                                    # FastAPI app entry point
‚îÇ ‚îú‚îÄ‚îÄ rag_pipeline.py                                            # RAG graph / LangChain integration
‚îÇ ‚îú‚îÄ‚îÄ stt.py                                                     # Whisper STT
‚îÇ ‚îú‚îÄ‚îÄ tts.py                                                     # pyttsx3 TTS or gTTS
‚îÇ
‚îú‚îÄ‚îÄ frontend/                                                    # React frontend
‚îÇ ‚îú‚îÄ‚îÄ public/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ index.html
‚îÇ ‚îú‚îÄ‚îÄ src/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ components/
‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ animations/
‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Mascot.js
‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Mascot.css
‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Controls.js
‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Controls.css
‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ StatusIndicator.js
‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ StatusIndicator.css
‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ LipSyncHandler.js                                    # Optional advanced animation handling
‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ TypingIndicator.js                                   # Optional typing animation
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ utils/
‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ speechUtils.js                                         # Browser STT/TTS utilities
‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ apiUtils.js                                            # API call utilities
‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ tts.js                                                 # Optional TTS handler
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ App.js
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ App.css
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ index.js
‚îÇ ‚îú‚îÄ‚îÄ .env                                                       # REACT_APP_API_URL pointing to backend
‚îÇ ‚îú‚îÄ‚îÄ package.json
‚îÇ ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ vector_store/                                                # FAISS vector DB files
‚îú‚îÄ‚îÄ knowledge_base/                                              # Embedded documents / knowledge files
‚îú‚îÄ‚îÄ .env                                                         # Backend environment config
‚îú‚îÄ‚îÄ requirements.txt                                             # Python dependencies for backend
‚îî‚îÄ‚îÄ README.md
```

## Workflow Architecture

```mermaid
flowchart TD

    subgraph User["üßë User"]
        Mic["üé§ Speak Question"]
        Mascot["ü§ñ Mascot Animation UI"]
    end

    subgraph Frontend["üíª Frontend (React)"]
        STT_Browser["üó£Ô∏è Browser STT (speechUtils.js)"]
        API_Calls["üì° API Calls (apiUtils.js)"]
        TTS_Browser["üîä Browser TTS"]
    end

    subgraph Backend["‚ö° Backend (FastAPI)"]
        STT["üéôÔ∏è Whisper STT (stt.py)"]
        RAG["üìö RAG Pipeline (LangChain + FAISS)"]
        TTS["üó£Ô∏è pyttsx3 TTS (tts.py)"]
    end

    subgraph KnowledgeBase["üìÇ Knowledge Base"]
        Docs["üìÑ Embedded Docs"]
        VDB["üóÑÔ∏è FAISS Vector Store"]
    end

    %% User Flow
    Mic --> STT_Browser --> API_Calls --> STT
    STT --> RAG
    RAG --> VDB
    VDB --> RAG
    RAG --> TTS
    TTS --> API_Calls --> TTS_Browser --> Mascot
```

## ‚öôÔ∏è Local Setup  

### 1Ô∏è. Backend (FastAPI)  

```bash
# Clone repository
git clone https://github.com/yourusername/conversational-ai-tutor.git
cd conversational-ai-tutor/backend

# Create virtual environment
python -m venv venv
source venv/bin/activate   # (Linux/Mac)
venv\Scripts\activate      # (Windows)

# Install dependencies
pip install -r requirements.txt

# Run FastAPI backend
uvicorn main:app --reload --port 8000
```
Backend will run at: http://127.0.0.1:8000
Swagger Docs: http://127.0.0.1:8000/docs

### 2. Frontend (React)
```bash
cd conversational-ai-tutor/frontend

# Install dependencies
npm install

# Start frontend
npm start
```

Frontend will run at: http://localhost:3000

## Live Deployment

- Backend (Render): https://conversational-ai-tutor.onrender.com/
- Frontend (Vercel): https://conversational-ai-tutor.vercel.app/
  
## API Endpoints
1. Query Endpoint
POST https://conversational-ai-tutor.onrender.com/query

Headers:
```bash
Content-Type: application/json
```
Body Example:
```
{
  "question": "What is sustainable agriculture and how does it differ from the Green Revolution?"
}
```
Response Example:
```
{
  "answer": "Sustainable agriculture focuses on long-term ecological balance...",
  "emotion": "explaining"
}
```

2. Chat Endpoint
POST https://conversational-ai-tutor.onrender.com/chat

Supports multi-turn conversations.

3. API Docs
Swagger API Docs available at:
```
https://conversational-ai-tutor.onrender.com/docs
```

## Testing with Postman
1. Open Postman.
2. Create a POST request to:
   - https://conversational-ai-tutor.onrender.com/query
   - https://conversational-ai-tutor.onrender.com/chat
3. Add Header: Content-Type: application/json.
4. Add request Body with JSON question.
5. Click Send ‚Üí Get AI Tutor response.

## Mascot UI Flow
1. Click üé§ Mic Button ‚Üí Speak a question.
2. Browser STT transcribes ‚Üí sends to Backend.
3. Backend (RAG) retrieves & generates response.
4. Returns { "answer": "...", "emotion": "happy" }.
5. Mascot speaks the response with lip-sync + emotion.


## Future Enhancements
1. Integrate advanced TTS (e.g., ElevenLabs / Coqui TTS for natural voices).
2. Add support for multiple subjects/domains in knowledge base.
3. Improve mascot animations with real-time lip-sync.
4. Add authentication and user history tracking.








