# ğŸ“ Conversational AI Tutor  

An **AI-powered conversational tutor** with **RAG backend, Speech-to-Text (STT), Text-to-Speech (TTS), and an animated mascot frontend**.  
This project was built as part of an **AI assignment challenge** to demonstrate **end-to-end AI system design, API integration, and deployment**.  

ğŸ”— **Backend API**: [FastAPI Backend on Render](https://conversational-ai-tutor.onrender.com/)  

---

## ğŸ“Œ Objective  

Build a **Conversational AI Tutor** that can:  
- Answer user queries using **RAG (Retrieval Augmented Generation)**.  
- Listen to the user via **speech recognition (STT)**.  
- Respond with voice via **text-to-speech (TTS)**.  
- Animate a mascot avatar with **lip-sync & emotions**.  

---

## âš¡ Features  

âœ… **Backend (FastAPI + LangChain RAG + FAISS Vector DB)**  
- `/query` â†’ Single query answering  
- `/chat` â†’ Multi-turn conversational memory  
- Returns **answer + emotion state** (happy, thinking, explaining)  

âœ… **Speech Recognition (STT)**  
- Implemented using **OpenAI Whisper**  

âœ… **Text-to-Speech (TTS)**  
- Implemented using **pyttsx3 (offline TTS)**  

âœ… **Frontend (React)**  
- Animated **Mascot UI**  
- **Mic button** â†’ Speak â†’ STT â†’ Backend â†’ Mascot speaks & animates  
- Supports **lip-sync and emotion-based facial animations**  

âœ… **Deployment**  
- Backend deployed on **Render**  
- Frontend deployed on **Vercel**  

---

## ğŸ› ï¸ Tech Stack  

- **Backend**: FastAPI, LangChain, FAISS, Whisper (STT), pyttsx3 (TTS)  
- **Frontend**: React.js, Web Audio API, Lottie/Canvas animations  
- **Deployment**: Render (Backend), Vercel (Frontend)  

---

## ğŸ“‚ Project Structure  
```
conversational-ai-tutor/
â”œâ”€â”€ backend/ # FastAPI backend
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ main.py # FastAPI app entry point
â”‚ â”œâ”€â”€ rag_pipeline.py # RAG graph / LangChain integration
â”‚ â”œâ”€â”€ stt.py # Whisper STT
â”‚ â”œâ”€â”€ tts.py # pyttsx3 TTS
â”‚
â”œâ”€â”€ frontend/ # React frontend
â”‚ â”œâ”€â”€ public/
â”‚ â”‚ â””â”€â”€ index.html
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ components/
â”‚ â”‚ â”‚ â”œâ”€â”€ animations/
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ Mascot.js
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ Mascot.css
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ Controls.js
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ Controls.css
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ StatusIndicator.js
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ StatusIndicator.css
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ LipSyncHandler.js # Optional advanced animation handling
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ TypingIndicator.js # Optional typing animation
â”‚ â”‚ â”œâ”€â”€ utils/
â”‚ â”‚ â”‚ â”œâ”€â”€ speechUtils.js # Browser STT/TTS utilities
â”‚ â”‚ â”‚ â”œâ”€â”€ apiUtils.js # API call utilities
â”‚ â”‚ â”‚ â””â”€â”€ tts.js # Optional TTS handler
â”‚ â”‚ â”œâ”€â”€ App.js
â”‚ â”‚ â”œâ”€â”€ App.css
â”‚ â”‚ â””â”€â”€ index.js
â”‚ â”œâ”€â”€ .env # REACT_APP_API_URL pointing to backend
â”‚ â”œâ”€â”€ package.json
â”‚ â””â”€â”€ README.md
â”‚
â”œâ”€â”€ vector_store/ # FAISS vector DB files
â”œâ”€â”€ knowledge_base/ # Embedded documents / knowledge files
â”œâ”€â”€ .env # Backend environment config
â”œâ”€â”€ requirements.txt # Python dependencies for backend
â””â”€â”€ README.md
```

## Workflow Architecture

```mermaid
flowchart TD

    subgraph User["ğŸ§‘ User"]
        Mic["ğŸ¤ Speak Question"]
        Mascot["ğŸ¤– Mascot Animation UI"]
    end

    subgraph Frontend["ğŸ’» Frontend (React)"]
        STT_Browser["ğŸ—£ï¸ Browser STT (speechUtils.js)"]
        API_Calls["ğŸ“¡ API Calls (apiUtils.js)"]
        TTS_Browser["ğŸ”Š Browser TTS"]
    end

    subgraph Backend["âš¡ Backend (FastAPI)"]
        STT["ğŸ™ï¸ Whisper STT (stt.py)"]
        RAG["ğŸ“š RAG Pipeline (LangChain + FAISS)"]
        TTS["ğŸ—£ï¸ pyttsx3 TTS (tts.py)"]
    end

    subgraph KnowledgeBase["ğŸ“‚ Knowledge Base"]
        Docs["ğŸ“„ Embedded Docs"]
        VDB["ğŸ—„ï¸ FAISS Vector Store"]
    end

    %% User Flow
    Mic --> STT_Browser --> API_Calls --> STT
    STT --> RAG
    RAG --> VDB
    VDB --> RAG
    RAG --> TTS
    TTS --> API_Calls --> TTS_Browser --> Mascot
```

## âš™ï¸ Local Setup  

### 1ï¸âƒ£ Backend (FastAPI)  

```bash
# Clone repository
git clone https://github.com/yourusername/conversational-ai-tutor.git
cd conversational-ai-tutor/backend

# Create virtual environment
python -m venv venv
source venv/bin/activate   # (Linux/Mac)
venv\Scripts\activate      # (Windows)

# Install dependencies
pip install -r requirements.txt

# Run FastAPI backend
uvicorn main:app --reload --port 8000
```
Backend will run at: http://127.0.0.1:8000
Swagger Docs: http://127.0.0.1:8000/docs

2ï¸âƒ£ Frontend (React)
cd conversational-ai-tutor/frontend

# Install dependencies
npm install

# Start frontend
npm start


Frontend will run at: ğŸ‘‰ http://localhost:3000

ğŸŒ Live Deployment

Backend (Render): https://conversational-ai-tutor.onrender.com/

Frontend (Vercel): https://conversational-ai-tutor.vercel.app/

ğŸ“¡ API Endpoints
ğŸ”¹ 1. Query Endpoint

POST https://conversational-ai-tutor.onrender.com/query

Headers:

Content-Type: application/json


Body Example:

{
  "question": "What is sustainable agriculture and how does it differ from the Green Revolution?"
}


Response Example:

{
  "answer": "Sustainable agriculture focuses on long-term ecological balance...",
  "emotion": "explaining"
}

ğŸ”¹ 2. Chat Endpoint

POST https://conversational-ai-tutor.onrender.com/chat

Supports multi-turn conversations.

ğŸ”¹ 3. API Docs

Swagger API Docs available at:
ğŸ‘‰ https://conversational-ai-tutor.onrender.com/docs

ğŸ§ª Testing with Postman

Open Postman.

Create a POST request to:

https://conversational-ai-tutor.onrender.com/query

https://conversational-ai-tutor.onrender.com/chat

Add Header: Content-Type: application/json.

Add request Body with JSON question.

Click Send â†’ Get AI Tutor response.

ğŸ­ Mascot UI Flow

Click ğŸ¤ Mic Button â†’ Speak a question.

Browser STT transcribes â†’ sends to Backend.

Backend (RAG) retrieves & generates response.

Returns { "answer": "...", "emotion": "happy" }.

Mascot speaks the response with lip-sync + emotion.

